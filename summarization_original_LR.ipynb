{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title Generation with T5 - Text Summarization\n",
    "### With parameters from the original Transformers paper: \"Attention is all you need\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Causal language modeling: the model has to predict the next token in the sentence (so the labels are the same as the inputs shifted to the right). To make sure the model does not cheat, its attention computations are masked so that tokens cannot attend to tokens to their right, as this would result in label leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from huggingface_hub import notebook_login\n",
    "rouge_score = evaluate.load(\"rouge\")\n",
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM, AdamWeightDecay, pipeline\n",
    "from transformers import DefaultDataCollator, DataCollatorForSeq2Seq\n",
    "import tensorflow as tf\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from clr_callback import *\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "pio.renderers.default = 'notebook_connected'\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the baseline model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")#, pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try a dataset longer text to resume - 512 max model length\n",
    "data = load_dataset(\"CShorten/ML-ArXiv-Papers\", split='train') #set to 100000\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [len(x.split()) for x in data[\"title\"]]\n",
    "px.histogram(words, nbins=100, text_auto=True, labels={\"value\":\"Title Length (words)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts = [len(x.split()) for x in data[\"abstract\"]]\n",
    "px.histogram(abstracts, nbins=400, marginal=\"rug\", labels={\"value\":\"Abstract Length (words)\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot=0\n",
    "for x in data[\"title\"]:\n",
    "    tot+=len(x.split())\n",
    "tot/117592"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot=0\n",
    "for x in data[\"abstract\"]:\n",
    "    tot+=len(x.split())\n",
    "tot/117592"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into Train, Test, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data.train_test_split(shuffle = True, seed = 200, test_size=0.2)\n",
    "validation = train['test'].train_test_split(shuffle = True, seed = 200, test_size=0.5)\n",
    "\n",
    "data_set = DatasetDict({\n",
    "    'train': train['train'],\n",
    "    'test': validation['train'],\n",
    "    'validation': validation['test']})\n",
    "\n",
    "data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Data with HF Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(data):\n",
    "    model_inputs = tokenizer(data[\"abstract\"], max_length=300, truncation=True)     #, padding=\"max_length\")\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(data[\"title\"], max_length=17, truncation=True)   #, padding=\"max_length\")\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = data_set.map(tokenization, batched = True, num_proc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized.remove_columns([\"Unnamed: 0\", \"Unnamed: 0.1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Train and Val Sets to TF Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use DataCollatorForSeq2Seq to create a batch of examples. It will also dynamically pad your text and labels to the length of the longest element in its batch, so they are a uniform length. While it is possible to pad your text in the tokenizer function by setting padding=True, dynamic padding is more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we convert our datasets to `tf.data.Dataset`, which Keras understands natively. There are two ways to do this - we can use the slightly more low-level [`Dataset.to_tf_dataset()`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset) method, or we can use [`Model.prepare_tf_dataset()`](https://huggingface.co/docs/transformers/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset). The main difference between these two is that the `Model` method can inspect the model to determine which column names it can use as input, which means you don't need to specify them yourself. It also supplies a data collator by default which is appropriate for most tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #16\n",
    "\n",
    "train_dataset = model.prepare_tf_dataset(\n",
    "    tokenized[\"train\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator\n",
    ")\n",
    "\n",
    "validation_dataset = model.prepare_tf_dataset(\n",
    "    tokenized[\"validation\"],\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling, Fitting, and Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've done that, it's time for our optimizer! We can initialize our `AdamWeightDecay` optimizer directly, or we can use the [`create_optimizer`](https://huggingface.co/docs/transformers/main_classes/optimizer_schedules#transformers.create_optimizer) function to generate an `AdamWeightDecay` optimizer with a learning rate schedule. In this case, we'll just stick with a constant learning rate for simplicity, so let's just use `AdamWeightDecay`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler used in the original transformer paper\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "  def __init__(self, d_model, warmup_steps=4000):\n",
    "    super().__init__()\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "    self.warmup_steps = warmup_steps\n",
    "\n",
    "  def __call__(self, step):\n",
    "    step = tf.cast(step, dtype=tf.float32)\n",
    "    arg1 = tf.math.rsqrt(step)\n",
    "    arg2 = step * (self.warmup_steps ** -1.5)\n",
    "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "  def get_config(self):\n",
    "        config = {\n",
    "            'd_model':self.d_model,\n",
    "            'warmup_steps':self.warmup_steps\n",
    "        }\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight decay is an L2 regularization\n",
    "# The parameters are taken from the transformers original paper\n",
    "# According to the T5 model config, d_model=512\n",
    "lr = CustomSchedule(d_model=512)\n",
    "optimizer = AdamWeightDecay(learning_rate=lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is optional\n",
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "\n",
    "model_name = \"T5\"\n",
    "push_to_hub_model_id = f\"{model_name}-finetuned-abstracts-oriLR\"\n",
    "\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"./clm_model_save_oriLR\",\n",
    "    tokenizer=tokenizer,\n",
    "    hub_model_id=push_to_hub_model_id,\n",
    "    hub_token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell is optional\n",
    "from numpy import histogram\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir=\"./tensorboard_oriLR\", \n",
    "                                    update_freq=1,\n",
    "                                    histogram_freq=1,\n",
    "                                    profile_batch=\"2,10\"\n",
    "                                    )\n",
    "\n",
    "callbacks = [tensorboard_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_dataset, validation_data=validation_dataset, epochs=3, workers=9, use_multiprocessing=True, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model and get its cross-entropy loss on the val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = model.evaluate(validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rouge(eval_set, max_length, min_length, model, tokenizer, n_iter=None):\n",
    "    summarizer = pipeline(\n",
    "        \"summarization\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        framework=\"tf\"\n",
    "    )\n",
    "    if n_iter == None:\n",
    "        n_iter = len(eval_set)\n",
    "    decoded_preds = []\n",
    "    decoded_labels = []\n",
    "\n",
    "    # avoid to load from cache, recompute shuffle\n",
    "    eval_set = eval_set.shuffle(seed=None, load_from_cache_file=False)\n",
    "\n",
    "    for i in tqdm(range(n_iter)):\n",
    "        summary = summarizer(eval_set[\"abstract\"][i], max_length=max_length, min_length=min_length)\n",
    "        decoded_preds.append(summary[0][\"summary_text\"])\n",
    "        labels = re.sub(\"\\n\",\"\", eval_set[\"title\"][i])\n",
    "        decoded_labels.append(labels)\n",
    "       \n",
    "\n",
    "    result = rouge_score.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results references from: https://paperswithcode.com/sota/abstractive-text-summarization-on-cnn-daily\n",
    "# computed on all test dataset \n",
    "compute_rouge(tokenized[\"test\"], 12, 8, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computed on subset\n",
    "compute_rouge(tokenized[\"test\"], 13, 5, model, tokenizer, n_iter=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Text Using a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"The vision proposed by the Semantic Web and the use of Linked Data allows for large-scale merging and integration  of data, thus giving access to a large amount of information. All this requires open standards and interoperability, properties not easy to achieve. The difficulty when integrating data resides in the fact that databases rarely adopt the same attributes to represent the same objects.\n",
    "As a result, an object is represented differently in different databases.\n",
    "To overcome this problem we need to add new definitions to the data. This new information is known as vocabulary (or ontology). Consequently, standardization of different vocabularies is also a practical difficulty.\n",
    "The benefits, however, concern greater efficiency in terms of search: the more data connections, the richer results are obtained. In addition, the schema flexibility of technologies like RDF  is an advantage over relational databases, where a change to the schema can pose difficulties. Other advantages include scalability and speed.\n",
    "Another important property of linked data is called inference and refers to the ability to infer new connections between data from the existing ones.\"\"\"\n",
    "\n",
    "res = summarizer(text, max_length=100, min_length=20)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pushing Up Stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import Repository\n",
    "repo = Repository(local_dir=\"./clm_model_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.git_pull()\n",
    "repo.push_to_hub(commit_message=\"Commit my-awesome-file to the Hub\")\n",
    "repo.git_push()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer.encode(text, return_tensors=\"tf\")\n",
    "output = model.generate(input_ids, max_length=50, no_repeat_ngram_size=2, early_stopping=True)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a model from the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EdBianchi/T5-finetuned-abstracts\")\n",
    "model = TFAutoModelForSeq2SeqLM.from_pretrained(\"EdBianchi/T5-finetuned-abstracts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"The vision proposed by the Semantic Web and the use of Linked Data allows for large-scale merging and integration  of data, thus giving access to a large amount of information. All this requires open standards and interoperability, properties not easy to achieve. The difficulty when integrating data resides in the fact that databases rarely adopt the same attributes to represent the same objects.\n",
    "As a result, an object is represented differently in different databases.\n",
    "To overcome this problem we need to add new definitions to the data. This new information is known as vocabulary (or ontology). Consequently, standardization of different vocabularies is also a practical difficulty.\n",
    "The benefits, however, concern greater efficiency in terms of search: the more data connections, the richer results are obtained. In addition, the schema flexibility of technologies like RDF  is an advantage over relational databases, where a change to the schema can pose difficulties. Other advantages include scalability and speed.\n",
    "Another important property of linked data is called inference and refers to the ability to infer new connections between data from the existing ones.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer1 = pipeline(\n",
    "    \"summarization\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    framework=\"tf\"\n",
    ")\n",
    "\n",
    "res = summarizer1(prompt, max_length=12, min_length=8)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c03b177c35db13f24209bdf9fef27ead8b5eeb38b2fe0f41bed77a06ac9b3ea9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
